import spacy
import re
import torch
from transformers import AutoTokenizer, AutoModelForParsing

# Load the BERT-based model and tokenizer
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForParsing.from_pretrained(model_name)

# Set up the spacy pipeline to handle multi-word entities
nlp = spacy.load('en_core_web_sm')
nlp.tokenizer.add_special_case('Boston Consultancy Group', [{'ORTH': 'Boston Consultancy Group', 'POS': 'PROPN'}])

# Define a function to perform dependency parsing using BERT
def bert_dep_parse(text):
    # Tokenize the input text
    input_ids = tokenizer.encode(text, return_tensors='pt')
    
    # Perform dependency parsing using the BERT-based model
    with torch.no_grad():
        outputs = model(input_ids)
        predicted_dependencies = outputs.logits.argmax(dim=-1).squeeze(0).cpu().numpy()

    # Get the list of tokens from the input text
    doc = nlp(text)
    tokens = [token.text for token in doc]

    # Combine multi-word entities into one token in the list of tokens
    pattern = re.compile(r'([^\W\d_]+(?: [^\W\d_]+)*)')
    tokens = [match.group().replace(' ', '_') if match.group().count(' ') > 0 else match.group() for match in pattern.finditer(text)]
    
    # Get the dependency labels and combine them with the corresponding tokens
    dependencies = [(tokens[dep[0]-1], dep[1]) for dep in enumerate(predicted_dependencies, start=1)]
    
    return dependencies
